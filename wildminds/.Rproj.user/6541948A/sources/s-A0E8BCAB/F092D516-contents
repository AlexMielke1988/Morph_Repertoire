#' Use unconditional and conditional probabilities of elements in bigram to determine which ones have above-expected levels of association
#'
#' Input is a list where every item contains all gestures in a sequence.
#'
#' @param elem.bout list of sequences as vectors, as produced by the detect_sequences() function
#' @param it number of iterations in case some elements overlap; overlapping elements should be separated with percentage sign; only necessary if these elements exist
#'
#' @return Function returns a data frame with the antecedent and consequent and the strength of their connection based on Fisher's exact and Chi-square test
#'
#' @importFrom dplyr mutate arrange desc as_tibble select all_of na_if left_join bind_rows bind_cols rename mutate_all summarise n ungroup rowwise
#' @importFrom stringr str_detect str_split str_replace_all str_to_title str_to_lower
#' @importFrom magrittr %>%
#' @importFrom tidyr replace_na gather separate unite
#' @importFrom tidytext unnest_tokens
#' @importFrom stats chisq.test complete.cases dist fisher.test frequency median na.omit quantile anova
#' @importFrom rlang .data
#'
#' @export
#'
#'

bigram_analytical <- function(elem.bout, it = 1) {
  # take bout of sequence vectors, simplify element names and remove additional symbols
  elem.bout <- elem.bout %>%
    lapply(str_replace_all,
           pattern = "-|/|_",
           replacement = ""
    )

  # remove NAs from sequences by artificially splitting vectors at NA, turn into multiple sequences within
  elem.bout <- lapply(elem.bout, function(x) {
    # split at NA
    rl <- rle(is.na(x))
    i1 <- rl$lengths == 1 & rl$values
    lst <-
      split(x, rep(cumsum(c(TRUE, i1[-length(i1)])), rl$lengths))
    lst <- lapply(lst, function(y) {
      y[complete.cases(y)]
    })
    return(lst)
  })

  # unlist so that new sequences are part of the same structure
  elem.bout <- unlist(elem.bout, recursive = FALSE)

  # add NAs at end of each sentence
  elem.bout <- lapply(elem.bout, function(x) {
    c(x, rep(NA, 1))
  })

  # if 'it' is defined as longer than 1, repeat the list as many times
  elem.bout.rep <- rep(elem.bout, it)

  # use unlist_list help function to separate overlapping elements that are connected by '%'; paste as string
  elems <-
    sapply(unlist_list(elem.bout.rep, method = "random"),
           paste,
           collapse = " "
    ) # further processing

  # split elementes within vectors using tidytext
  elems_split <- elems %>%
    as_tibble() %>%
    unnest_tokens(.data$words,
                  .data$value,
                  to_lower = FALSE
    ) %>%
    na_if('NA')

  # take bigrams and turn into data frame of distinct combinations of antecedent and consequent
  elems_words <- elems_split %>%
    rename(word1 = .data$words) %>%
    mutate(word2 = c(.data$word1[2:length(.data$word1)], NA)) %>%
    drop_na()

  # create count of all bigrams and clean up
  elems2grams <- elems_words %>%
    mutate(bigram = paste(.data$word1, .data$word2, sep = " ")) %>%
    group_by(.data$bigram) %>%
    summarise(frequency = n()) %>%
    arrange(-.data$frequency) %>%
    mutate(count = .data$frequency / it) %>%
    select(-.data$frequency) %>%
    separate(
      .data$bigram,
      into = c("item1", "item2"),
      sep = " ",
      remove = TRUE
    )

  # create adjacency matrix for the different bigrams
  matrix.first <- elems2grams %>%
    acast(item1 ~ item2,
          value.var = "count",
          drop = TRUE,
          fill = 0,
          fun.aggregate = sum
    ) %>%
    replace_na(0) %>%
    as.matrix() %>%
    data.frame()

  # create conditional and unconditional probabilities, use Chisquare and Fisher's test to establish significance
  significance.table <-
    # count bigram and individual elements
    matrix.first %>%
    mutate(antecedent = rownames(matrix.first)) %>%
    gather(
      consequent,
      observed.sum,
      colnames(matrix.first)[1]:colnames(matrix.first)[ncol(matrix.first)]
    ) %>%
    mutate(
      antecedent = factor(.data$antecedent),
      consequent = factor(.data$consequent)
    ) %>%
    mutate(AllFreq = sum(.data$observed.sum)) %>%
    group_by(.data$antecedent) %>%
    mutate(antecedent.count = sum(.data$observed.sum)) %>%
    ungroup(.data$antecedent) %>%
    group_by(.data$consequent) %>%
    mutate(consequent.count = sum(.data$observed.sum)) %>%
    arrange(.data$antecedent) %>%
    # create counts for significance tests
    mutate(
      a = .data$observed.sum,# observed bigram
      b = .data$antecedent.count - .data$a, # antecedent without bigram
      c = .data$consequent.count - .data$a, # consequent without bigram
      d = .data$AllFreq - (.data$a + .data$b + .data$c) # all cases without the three
    ) %>%
    # calculate how many elements
    mutate(NRows = nrow(matrix.first)) %>%
    rowwise() %>%
    # calculate p-value based on Fisher's exact test for count data
    mutate(p = as.vector(unlist(fisher.test(
      matrix(c(.data$a, .data$b, .data$c, .data$d),
             ncol = 2, byrow = T
      )
    )[1]))) %>%
    # calculate X2 based on Pearson's Chi-squared Test for Count Data
    mutate(x2 = as.vector(unlist(chisq.test(
      matrix(c(.data$a, .data$b, .data$c, .data$d), ncol = 2, byrow = T)
    )[1]))) %>%
    # calculate Phi
    mutate(phi = sqrt((.data$x2 / (.data$a + .data$b + .data$c + .data$d)))) %>%
    # calculate expected value under independence
    mutate(expected = as.vector(unlist(chisq.test(
      matrix(c(.data$a, .data$b, .data$c, .data$d), ncol = 2, byrow = T)
    )$expected[1]))) %>%
    # calculate change in probability over expected
    mutate(prob.increase = .data$observed.sum / .data$expected) %>%
    ungroup() %>%
    arrange(desc(.data$prob.increase)) %>%
    mutate(p = round(.data$p, 4)) %>%
    mutate(x2 = round(.data$x2, 2)) %>%
    mutate(phi = round(.data$phi, 2)) %>%
    select(-.data$a, -.data$b, -.data$c, -.data$d, -.data$NRows, -.data$AllFreq) %>%
    suppressMessages()

  return(significance.table)
}
